{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download magazine articles per subject"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook guides you through the SRU and OAI of the KB: National Library of the Netherlands, in order to collect magazine articles based on subject and time range. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install the neccesary packages\n",
    "\n",
    "It is preffered to install the package through a commandline, but installing through the Jupypter Notebook is also possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas\n",
    "%pip install requests\n",
    "%pip install BeautifulSoup4\n",
    "%pip install lxml\n",
    "%pip install html5lib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import  the neccesary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the necessary packages \n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import xml\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the API key\n",
    "\n",
    "An API key is needed to query and download material. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apikey = \"\" #Insert the API key here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the search parameters\n",
    "\n",
    "There are various parameters that can be used to search through the collection.\n",
    "The code in this notebook is based on searching with a keyword.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = 'Inhuldiging+and+koning' ## use '+or+' or '+and+' to search with multiple keywords, such as 'griep+and+ziekte'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving the magazine article identifiers\n",
    "\n",
    "Before we can download the actual content, we need a list of identifiers from the magazine articles that fit to the selection criteria we made above. We put this list in a dataframe in which we store some additional metadata. This  dataframe is used later on for accessing the content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extract the identifiers\n",
    "## This might take a while\n",
    "identifierList = []\n",
    "startRecord = 0\n",
    "maximumRecord = 1000\n",
    "recordCounter = 0\n",
    "\n",
    "## Assemble the query based on the parameters, we set the  maximumRecords to 1000 to prevent overloading the system\n",
    "query = f\"https://jsru.kb.nl/sru/sru/{apikey}?operation=searchRetrieve\"\\\n",
    "        f\"&query={keyword}\"\\\n",
    "        f\"&recordSchema=dc&startRecord={startRecord}&maximumRecords={maximumRecord}&x-collection=DTS_pagina\"\n",
    "print(query)\n",
    "\n",
    "\n",
    "page = requests.get(query)\n",
    "soup = BeautifulSoup(page.content,'xml')\n",
    "print(soup)\n",
    "\n",
    "for item in soup.findAll('srw:searchRetrieveResponse'):\n",
    "    records = item.find('srw:numberOfRecords').text\n",
    "    \n",
    "## Iterate through the query results to extract the metadata \n",
    "while recordCounter < int(records):\n",
    "    page = requests.get(query)\n",
    "    soup = BeautifulSoup(page.content, 'xml')\n",
    "\n",
    "    ## The query returns an xml page with (in this example) 1000 articles \n",
    "    ## We extract the metadate per article\n",
    "    for item in soup.findAll('srw:recordData'):\n",
    "        identifier = item.find('dcx:pageOcrUrl')\n",
    "        oai = item.find('OaiPmhIdentifier')    \n",
    "        identifierList.append([identifier.text, oai.text])\n",
    "        recordCounter += 1\n",
    "    ## If there are more than 1000 results, \n",
    "    ## this code is used to proceed to the next pages to collect the remainder of the results\n",
    "    startRecord = startRecord + 1000\n",
    "    query = f\"https://jsru.kb.nl/sru/sru/{apikey}?operation=searchRetrieve\"\\\n",
    "        f\"&query={keyword}\"\\\n",
    "        f\"&recordSchema=dc&startRecord={startRecord}&maximumRecords={maximumRecord}&x-collection=DTS_pagina\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create the dataframe\n",
    "dfIdentifiers = pd.DataFrame(identifierList, columns = ['identifier', 'oai'])\n",
    "## Show the number of found identifiers\n",
    "print('Found: ',len(dfIdentifiers))\n",
    "csvlist = dfIdentifiers.to_csv('identifierslist.csvlist')\n",
    "dfIdentifiers.head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieving the magazine identifiers\n",
    "The articles do not directly include metadata on the magazines. Therefore, some additional coding is necessary to retrieve metadata of the magazine (in this case title and date) to which the article belongs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each record in the DataFrame to get the title and date\n",
    "for index, row in dfIdentifiers.iterrows():\n",
    "    oai = row['oai']  # Extract the OaiPmhIdentifier\n",
    "    identifier = row['identifier']\n",
    "    \n",
    "    # Construct the URL for DTS records\n",
    "    if \"DTS\" in oai or \"TIJDSCHRIFTEN\" in oai:\n",
    "        # Form the URL by replacing the 'DTS:' with the proper format for the URL\n",
    "        urn = identifier.split(\"urn=\")[-1]\n",
    "        urn = \":\".join(urn.split(\":\")[:3])\n",
    "        url = f\"http://resolver.kb.nl/resolve?urn={urn}\"\n",
    "        \n",
    "        # Try to access the URL and parse the title and extract date\n",
    "        try:\n",
    "            page = requests.get(url)\n",
    "            soup = BeautifulSoup(page.content, 'html.parser')  \n",
    "            title_tag = soup.find('title')  \n",
    "            if title_tag:\n",
    "                magazine_title = title_tag.text.strip() \n",
    "                dfIdentifiers.at[index, 'magazine title'] = magazine_title\n",
    "                date_match = re.search(r'\\b\\d{4}\\b', magazine_title)\n",
    "                if date_match:\n",
    "                    magazine_date = date_match.group(0)\n",
    "                else:\n",
    "                    magazine_date = \"No Date Found\"\n",
    "                dfIdentifiers.at[index, 'publication date'] = magazine_date\n",
    "            else:\n",
    "                dfIdentifiers.at[index, 'magazine title'] = \"No Title Found\"\n",
    "                dfIdentifiers.at[index, 'publication date'] = \"No Date Found\"\n",
    "        except Exception as e:\n",
    "            dfIdentifiers.at[index, 'magazine title'] = \"Error Fetching Title\"\n",
    "            dfIdentifiers.at[index, 'publication date'] = \"Error Fetching Date\"\n",
    "            print(f\"Error for {url}: {e}\")\n",
    "\n",
    "# Show the updated DataFrame with the magazine title and publication date\n",
    "print('Updated DataFrame with magazine titles and publication dates:')\n",
    "print(dfIdentifiers.head(40))\n",
    "\n",
    "# Save the updated DataFrame to the same CSV file, overwriting the original\n",
    "dfIdentifiers.to_csv('identifierslist.csvlist', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve the content of the articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieve the content of the articles based on the identifiers\n",
    "## If there are a lot of articles, this can take a while\n",
    "\n",
    "contentList = []\n",
    "\n",
    "for index, row in dfIdentifiers.iterrows():\n",
    "    identifier = row['identifier']\n",
    "    url = requests.get(identifier)\n",
    "\n",
    "    if url.status_code == 200:\n",
    "        soup = BeautifulSoup(url.content, \"xml\")\n",
    "        text = ''\n",
    "        for item in soup.findAll('p'):\n",
    "            text = text + (item.text)\n",
    "        contentList.append([identifier, text])\n",
    "    else:\n",
    "        contentList.append([identifier, \"Not enough rights to view digital object\"])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a dataframe\n",
    "dfText = pd.DataFrame(contentList, columns = ['identifier', 'content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dfText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfText.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfText[dfText['content'].str.contains('rel')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge the metadata with the content\n",
    "\n",
    "This is an additional step to store everything in one dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfArticles = dfIdentifiers.merge(dfText, on = 'identifier', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfArticles.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfArticles[dfArticles['content'].str.contains('rel')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfArticles = dfArticles.head(10)\n",
    "print(dfArticles.head(10))\n",
    "\n",
    "csvdfArticles = dfArticles.to_html('Articles.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
